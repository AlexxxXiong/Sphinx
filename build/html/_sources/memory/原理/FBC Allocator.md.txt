## [TensorFlow中的显存管理器——BFC Allocator ](https://www.cnblogs.com/deep-learning-stacks/p/10741859.html)

使用GPU训练时，一次训练任务无论是模型参数还是中间结果都需要占用大量显存。为了避免每次训练重新开辟显存带来计算之外的开销，一般框架的做法是在真正的训练任务开始前，将每个节点的输入和输出，以及模型参数的shape计算出来并全局开辟一次，例如Caffe就是这种做法。

随着深度学习模型的发展和迭代，不仅模型训练的数据shape可能发生变化，就连模型本身在训练过程中也可能发生变化，那么按照固定shape一次开辟显存的做法就不能满足需求了。

为此，TensorFlow重新设计了较为灵活的显存管理机制，它使用了名为BFC的分配算法，并通过BFC Allocator为每个Tensor分配满足需求的显存。本节我们将一起窥探BFC Allocator的设计思想。

### 从Tensor的创建谈起

#### 为Tensor分配存储区的时机

在进入主题之前，让我们先思考一个问题：TensorFlow中的Tensor究竟是何时拿到所需存储区的呢？**答案是在Tensor对象被创建时就立即进行分配。**

**在TensorFlow的一轮训练结束后，所有的Tensor都已经被释放，下一轮计算开始后会按照需求重新创建Tensor，并为其分配新的存储空间。**

下面的代码片段中我们可以看到Tensor创建时，使用Allocator分配存储区的代码段。

```c
'''
Allocator* a: 指向一个分配器对象的指针，该分配器用于管理内存分配。
DataType type: 表示张量的数据类型，如浮点数、整数等。
const TensorShape& shape: 表示张量的形状，即它的维度信息。
''' 

Tensor::Tensor(Allocator* a, DataType type, const TensorShape& shape)
 : shape_(shape), buf_(nullptr) {
set_dtype(type);
CHECK_NOTNULL(a);
if (shape_.num_elements() > 0 || a->ShouldAllocateEmptyTensors()) {  // 条件内存分配
 CASES(type, buf_ = new Buffer<T>(a, shape.num_elements()));  // 这一行是一个宏或模板，根据数据类型 type 创建一个适当类型的 Buffer<T> 对象，并初始化 buf_
}
if (buf_ != nullptr && buf_->data() != nullptr && LogMemory::IsEnabled()) {
 LogMemory::RecordTensorAllocation("Unknown", LogMemory::UNKNOWN_STEP_ID,   // 用于记录分配事件
                                   *this);
}
}

Template <typename T>
Buffer<T>::Buffer(Allocator* a, int64 n,
               const AllocationAttributes& allocation_attr)
 : BufferBase(a, a->Allocate<T>(n, allocation_attr)), elem_(n) {}
// 因为在此处调用了Allocate函数，此时Buffer真正获得了一片实际的存储区。这已经能够说明存储区分配的时机是在一个Tensor对象被创建时立即发生的。
```

以下是一些关键时刻，这些张量会被创建：

### 1. **模型定义时**

在定义 MLP 模型时，你需要指定模型的结构，包括各层的类型、大小和连接方式。此时，会创建与模型参数（权重和偏置）相关的张量。这些张量通常在模型的初始化阶段就已经定义好，并在训练过程中被优化。例如，每个全连接层（dense layer）都会有相应的权重和偏置张量。

### 2. **模型编译时**

当你编译 TensorFlow 模型（通过 `model.compile()` 调用），准备它们进行训练或推理时，会设置损失函数、优化器和评价指标。这一步骤可能不直接创建张量，但会准备必要的基础设施，例如梯度张量，这些张量用于在训练期间更新权重。

### 3. **加载模型数据时**

在进行推理之前，你需要加载或指定输入数据。输入数据在送入模型前通常被封装为一个张量。这是在推理或训练过程开始前的一步，确保所有输入数据都以正确的形式（尺寸和类型）被处理。

### 4. **执行推理时**

在模型推理（或称为前向传播）过程中，数据会通过模型的各层。每一层都会对输入数据执行计算，并生成输出数据，这些数据同样被存储在张量中。例如，一个典型的全连接层会计算 `output = activation(dot(input, kernel) + bias)`，其中 `input`, `kernel` (权重), 和 `bias` 都是张量，`output` 也会被存储为一个新的张量。

### 5. **后处理**

在得到最终的输出后，可能还需要对这些输出进行进一步的处理，如应用 softmax 函数来获取概率分布。这一步骤可能会创建新的张量来存储处理后的结果。

###  遇到的问题——显存分配与回收的性能需求

Tensor在每次创建时会得到存储区域，而每一轮训练都要重新创建新的Tensor，那么这里面临的一个问题：**如此频繁的分配和回收存储区，如何才能做的高效？**试想对于GPU来说，如果Allocate函数直接封装CUDA中昂贵的cudaMalloc函数，当Tensor被释放时直接调用cudaFree函数，那么训练速度将会因为这些overhead大打折扣。

