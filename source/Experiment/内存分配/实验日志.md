# 实验日志

-----

## 20240516

> 从前面实验我们知道使用pin监控malloc并不好。
>
> 1. 没有办法对应到推理行为。
> 2. 插桩导致的时间过长，不准确。
>
> 我们今天探索一下tensorflow/lite是如何实现profile的。

### TensorFlow Lite 内存管理回顾

TensorFlow Lite（TFLite）采用了一种称为“线性分配器”（Linear Allocator）的内存管理策略，这种策略基于在模型转换阶段进行的内存需求分析。具体来说，TFLite 通过以下步骤管理内存：

#### 1. 模型转换阶段的分析

在模型转换阶段（通常是从 TensorFlow 模型转换为 TFLite 模型时），TFLite 的转换器会对模型进行分析，确定每个张量的内存需求，包括：

- 张量的大小（内存需求量）。
- 张量的生命周期（分配时间和回收时间）。

这种分析会生成一个内存需求的“时间表”，描述每个张量在推理过程中的创建和销毁时间点。

#### 2. 线性分配器策略

基于上述分析结果，TFLite 采用线性分配器策略来进行内存管理。线性分配器的核心思想是：

- **预分配大块连续内存**：线性分配器会在模型加载时预分配一块大块的连续内存空间，用于存储所有的张量。
- **按需分配和释放**：根据模型转换阶段生成的**内存需求时间表**，线性分配器会在需要的时候分配内存，并在不再需要的时候释放内存。

这种策略的优点是：
- **减少内存碎片**：由于所有内存分配都在一个大的连续块中进行，可以有效减少内存碎片，提高内存利用率。
- **提高内存分配效率**：通过预先计算的内存需求，可以避免在推理过程中频繁的内存分配和释放操作，从而提高效率。

#### 3. 贪心算法

为了进一步优化内存利用，TFLite 在内存分配时会采用一些优化策略，例如使用贪心算法来决定张量的具体分配位置。贪心算法的目标是：

- 尽可能复用内存：在一个张量生命周期结束后，其占用的内存可以立即被其他新的张量复用，从而减少总体内存需求。

### TensorFlow Lite 内存需求时间表获得

在 TensorFlow Lite (TFLite) 中，内存需求分析在模型转换阶段进行，并生成每个张量的内存分配和释放时间表。这是通过分析模型的计算图 (graph) 来实现的。具体来说，TFLite 的转换器会遍历模型的计算图，确定每个操作 (operation) 需要的输入和输出张量的生命周期。

#### 内存需求分析的具体步骤

1. **遍历计算图**：TFLite 转换器会遍历模型的计算图，分析每个操作节点及其输入输出张量。
2. **确定张量大小**：计算每个张量所需的内存大小，这通常是根据张量的形状和数据类型来确定的。
3. **确定张量生命周期**：记录每个张量的创建和销毁时间点。这是通过分析模型中的操作依赖关系来实现的，即哪些操作会使用哪些张量，以及这些操作何时被执行。
4. **生成内存时间表**：根据张量的生命周期，生成一个内存需求的时间表，描述每个张量在推理过程中的创建和销毁时间点。

#### 具体实现的源码

在 TensorFlow Lite 的源码中，内存需求分析和内存分配的逻辑主要集中在以下几个文件和类中：

1. **`tensor_allocation.cc`**：这个文件包含了张量分配的核心逻辑，包括内存需求分析和张量的实际内存分配。
2. **`interpreter.cc`**：这个文件包含了 TFLite 解释器的实现，其中涉及了内存分配和管理的部分逻辑。
3. **`arena_planner.cc`**：这个文件实现了 TFLite 的内存分配器（即线性分配器）的逻辑，包括张量的分配和释放策略。



根据 `tree.txt` 文件中的目录结构，我们可以重点关注与内存需求时间表相关的几个文件和目录。以下是一些可能与内存需求时间表获取相关的文件和目录：

重点文件分析

#### 1. `arena_planner.cc` 和 `arena_planner.h`
这些文件通常包含内存规划的核心逻辑。

**ArenaPlanner::PlanAllocations**

```c++
TfLiteStatus ArenaPlanner::PlanAllocations() {
  // 函数 PlanAllocations 的返回类型是 TfLiteStatus，说明它会返回一个状态值。
  const size_t num_tensors = graph_info_->num_tensors();
  // 这里开始定义PlanAllocations函数，首先获取图形中张量的数量，赋值给num_tensors变量。
  
  // TF_LITE_ENSURE_STATUS(ResetAllocations()) 确保现有的分配状态被重置，并检查这个操作是否成功。
  TF_LITE_ENSURE_STATUS(ResetAllocations());

  // 使用 kNodeNotAssigned 初始化 alloc_node_ 和 dealloc_node_ 数组，长度为 num_tensors，表示所有张量初始时都没有被分配或释放。
  // kNodeNotAssigned 是一个特殊值，用来表示一个张量还没有被任何节点分配或释放。它被初始化为 int32_t 类型的最大值。
  // alloc_node_ 是一个向量（数组），记录每个张量第一次被哪个节点使用。该张量需要在这个节点的操作执行之前分配。比如，如果某个张量第一次被节点 5 使用，那么 alloc_node_[这个张量的索引] 就会是 5。
  // dealloc_node_ 也是一个向量，记录每个张量最后一次被哪个节点使用。在这个节点的操作执行之后，该张量就可以被释放。比如，如果某个张量最后一次被节点 10 使用，那么 dealloc_node_[这个张量的索引] 就会是 10。
  // alloc_node_ 和 dealloc_node_ 都被初始化为 kNodeNotAssigned，即所有张量初始状态下都没有被任何节点使用或释放。
  alloc_node_.assign(num_tensors, kNodeNotAssigned);
  dealloc_node_.assign(num_tensors, kNodeNotAssigned);
  
  // nodes_to_tensors_：用于存储每个节点对应的张量集合，初始状态下每个节点的张量集合都是空的
  // 调整 nodes_to_tensors_ 的大小，使其能够容纳至少一个执行节点，或 graph_info_->num_execution_nodes() 个节点，取较大的值。每个元素初始化为空的集合 {}。
  nodes_to_tensors_.clear();
  nodes_to_tensors_.resize(
      std::max(graph_info_->num_execution_nodes(), (size_t)1), {});

  // 初始化 refcounts_ 数组，将每个张量的引用计数设为 0。
  refcounts_.assign(num_tensors, 0);

  // 定义一个局部 lambda 函数 allocate，用于分配张量。
	// 如果张量已经被分配（alloc_node_[tensor] != kNodeNotAssigned），则直接返回 kTfLiteOk。
	// 确保该张量尚未被释放（dealloc_node_[tensor] == kNodeNotAssigned），然后将分配节点设置为当前节点。
  auto allocate = [this](int node, int tensor) -> TfLiteStatus {
    if (alloc_node_[tensor] != kNodeNotAssigned) {
      // Tensor has already been allocated.
      return kTfLiteOk;
    }
    // 这个宏确保张量在被分配之前没有被标记为已释放。如果张量已经被标记为释放，这意味着有逻辑错误，因为一个张量不能在释放后再次被分配。
    TF_LITE_ENSURE(context_, dealloc_node_[tensor] == kNodeNotAssigned);
    // 将当前节点（node）的索引记录为第一个分配该张量的节点。即标记这个节点为分配该张量的节点。
    alloc_node_[tensor] = node;
    return kTfLiteOk;
  };

  auto deallocate = [this](int node, int tensor) -> TfLiteStatus {
    if (alloc_node_[tensor] == kNodeNotAssigned) {
      // We don't need to deallocate the tensor, that is never allocated.
      // This happened with the constant tensors.
      return kTfLiteOk;
    }

    TF_LITE_ENSURE(context_, dealloc_node_[tensor] == kNodeNotAssigned);
    dealloc_node_[tensor] = node;
    return kTfLiteOk;
  };

  // We must make sure the output tensors are never overwritten. We do that by
  // artificially adding one to their ref-counts so they are never selected
  // for deallocation.
  // 遍历所有输出张量，确保其引用计数增加，以防止这些张量被覆盖或释放。 
  // kTfLiteOptionalTensor：这是一个特殊值，用来表示在某些情况下，一个张量是可选的。在这种情况下，该张量可能不存在，或者其值不重要。
	// 引用计数：用于跟踪某个资源（例如张量）被多少个地方使用。引用计数增加表示该资源被更多地方使用，需要保留。
  for (int tensor_index : graph_info_->outputs()) {
    if (tensor_index != kTfLiteOptionalTensor) {
      ++refcounts_[tensor_index];
    }
  }

  // Variable tensors also should be ensured to be never overwritten and need to
  // be alive all the time.
  for (int tensor_index : graph_info_->variables()) {
    // Increase the reference count for variable tensors by one, so it will
    // never be deallocated.
    // 增加每个变量张量的引用计数。这样可以确保这些变量张量在整个计算图的生命周期内不会被释放。
    ++refcounts_[tensor_index];
    // `variables` is a subgraph-level list and it should never be
    // kTfLiteOptionalTensor.
    // 确保每个变量张量的索引不是 kTfLiteOptionalTensor。这是一个安全检查，保证变量张量的索引有效。
    TF_LITE_ENSURE(context_, tensor_index != kTfLiteOptionalTensor);
    // Variable tensor should be allocated at the very beginning.
    // 调用 allocate 函数，在图的最开始（节点 0）分配变量张量。
    TF_LITE_ENSURE_STATUS(allocate(0, tensor_index));
    // 将当前变量张量的索引添加到第一个节点（节点 0）所分配的张量集合中。
    nodes_to_tensors_[0].insert(tensor_index);
  }

  // Queue all graph inputs for allocation and make sure they are never
  // overwritten.
  for (int tensor_index : graph_info_->inputs()) {
    if (tensor_index != kTfLiteOptionalTensor) {
      ++refcounts_[tensor_index];
      TF_LITE_ENSURE_STATUS(allocate(0, tensor_index));
      nodes_to_tensors_[0].insert(tensor_index);
    }
  }
  // Copy reference counts before sharing tensors so that the correct values are
  // used to determine if a tensor may be shared or not.
  // 备份当前的引用计数，方便后续操作中使用。
  std::vector<int> refcounts = refcounts_;
  // Count references to node input tensors.
  // 获取计算图中有多少个执行节点。
  const int num_execution_nodes = graph_info_->num_execution_nodes();
  // 遍历每个执行节点，获取每个节点的信息以及其输入张量的数组 node_inputs。
  for (size_t i = 0; i < num_execution_nodes; ++i) {
    const TfLiteNode& node = graph_info_->node(i);
    TfLiteIntArray* node_inputs = node.inputs;
    // 遍历当前节点的所有输入张量，如果输入张量不是可选张量（kTfLiteOptionalTensor），则增加该张量的引用计数。
    for (int j = 0; j < node_inputs->size; ++j) {
      int tensor_index = node_inputs->data[j];
      if (tensor_index != kTfLiteOptionalTensor) {
        ++refcounts_[tensor_index];
      }
    }
  }

  // 识别并标记那些可以就地（in-place）操作的张量。In-place 操作意味着在计算过程中，输出张量可以与输入张量共享相同的内存空间，从而节省内存使用。
  IdentifyInPlaceTensors();
  // Use the new reference counts to determine when tensors memory can safely be
  // reused.
  for (size_t i = 0; i < num_execution_nodes; ++i) {
    const TfLiteNode& node = graph_info_->node(i);
    TfLiteIntArray* node_inputs = node.inputs;
    for (int j = 0; j < node_inputs->size; ++j) {
      int tensor_index = node_inputs->data[j];
      if (tensor_index != kTfLiteOptionalTensor) {
        // Correctly count references for shared buffers.
        tensor_index = FindSharedTensor(tensor_index);
        ++refcounts[tensor_index];
      }
    }
  }
  // 这段代码的作用是遍历所有执行节点，首先分配每个节点的输出张量，然后根据引用计数更新输入张量的引用，并在必要时释放这些输入张量。它确保张量在计算图执行过程中被正确分配和释放。
  // Go through the graph in execution order.
  for (size_t i = 0; i < num_execution_nodes; ++i) {
    const TfLiteNode& node = graph_info_->node(i);

    // First queue output tensors for allocation.
    TfLiteIntArray* node_outputs = node.outputs;
    for (int j = 0; j < node_outputs->size; ++j) {
      int tensor_index = node_outputs->data[j];
      if (tensor_index == kTfLiteOptionalTensor) continue;
      //  Don't allocate output tensors here for shared memory parts.
      nodes_to_tensors_[i].insert(tensor_index);
      TF_LITE_ENSURE_STATUS(allocate(i, tensor_index));
    }

    // Then update the ref-counts of the node's inputs, and if necessary queue
    // them for deallocation.
    if (!preserve_all_tensors_) {
      TfLiteIntArray* node_inputs = node.inputs;
      for (int j = 0; j < node_inputs->size; ++j) {
        // If the tensor is a ref we decrement the original tensor.
        int tensor_index = node_inputs->data[j];
        if (tensor_index != kTfLiteOptionalTensor) {
          // Correctly count references for shared buffers.
          tensor_index = FindSharedTensor(tensor_index);
          --refcounts[tensor_index];
          if (refcounts[tensor_index] == 0) {
            TF_LITE_ENSURE_STATUS(deallocate(i, tensor_index));
          }
        }
      }
    }
  }
  // Note that graph outputs will never be scheduled for deallocation. We
  // could do that here for completeness, but it won't have any effect.
  return kTfLiteOk;
}
```

> ```c++
> 	•	GraphInfo 类：这是一个抽象类，提供了关于计算图的基本信息。
> 	•	虚析构函数：virtual ~GraphInfo() {}，确保子类可以正确析构。
> 	•	纯虚函数：
> 	•	num_tensors() const：返回图中的总张量数量。
> 	•	tensor(size_t index)：根据索引返回对应的张量。
> 	•	tensors()：返回所有张量的指针。
> 	•	num_execution_nodes() const：返回当前执行计划中的节点数量。
> 	•	num_total_nodes() const：返回已知的所有节点总数。
> 	•	node(size_t index) const：根据索引返回执行计划中的节点。
> 	•	registration(size_t index) const：根据索引返回节点的注册信息。
> 	•	node_index(size_t index) const：返回实现特定的节点索引。
> 	•	inputs() const：返回输入张量的索引。
> 	•	outputs() const：返回输出张量的索引。
> 	•	variables() const：返回变量张量的索引。
>     
>     
>  	•	NodeSubset 结构：表示计算图中节点的子集。
> 	•	枚举类型 Type：
> 	•	kTfUnexplored：创建过程中暂时使用。
> 	•	kTfPartition：表示一个划分的子集。
> 	•	kTfNonPartition：表示非划分的子集。
> 	•	成员变量：
> 	•	type：节点子集的类型，默认值为 kTfUnexplored。
> 	•	nodes：节点子集中的节点索引。
> 	•	input_tensors：依赖于其他节点子集的张量或全局输入张量。
> 	•	output_tensors：被其他节点子集消费的输出张量或全局输出张量。
>     
>     
>   •	作用：将一组节点索引 nodes_to_partition 划分为独立的节点子集。
> 	•	参数：
> 	•	info：包含计算图信息的 GraphInfo 对象。
> 	•	nodes_to_partition：要划分的节点索引列表。
> 	•	node_subsets：输出参数，用于存储划分后的节点子集。
> 	•	greedily：如果为 true，则贪心地划分节点子集。
> 	•	control_edges：控制依赖关系的有向无环图（可选）。
>     
>   功能：将节点索引列表 nodes_to_partition 划分成若干个节点子集。每个节点子集内部的节点按照依赖顺序排列，且节点子集之间也按依赖顺序排列。
> 	假设：计算图中的节点按照依赖关系已经排序。
>   greedily 为 true 时：在生成节点子集时，只要节点属于 *nodes_to_partition 并且可以调度（即它依赖的所有节点已经添加到 *node_subsets），就将节点添加到当前子集中。
> 	greedily 为 false 时：保留原始执行顺序，即生成的节点子集形式如 [ [0..i_1), [i1..i2), ... ]。
>   •	greedily 为 true：在生成节点子集时，尽可能多地将节点加入当前子集，只要这些节点符合条件（属于 nodes_to_partition 且依赖的所有节点已被添加）。
> 	•	greedily 为 false：严格保持原始的节点执行顺序，不做额外优化。
>   
>   control_edges 指定了节点的控制依赖关系有向无环图（DAG）。最终的节点划分将遵循这些控制依赖关系。
> 	作用：除了节点的数据依赖关系外，还可以对图的最终执行顺序施加限制（这在贪心划分时尤为重要）。
>       
>                                              
> // (Example: with `greedily`, `control_edges.empty()`, and `nodes_to_partition
> // == {2, 3}`, the graph
> //
> //                    /------------\
> //                    |            v
> // 0 --> 1 --> 2* --> 3*     4 --> 5
> //       |                   ^
> //       \-------------------/
> //
> // will be partitioned as {{0, 1, 4}, {2, 3}, {5}}, since data dependencies
> // (notated '-->') allow for execution of 4 immediately after 1.
> //
> // With an additional control dependency `control_edges == {{3, 4}}` (notated
> // '==>'), execution of node 4 requires prior execution of node 3:
> //
> //                    /------------\
> //                    |            v
> // 0 --> 1 --> 2* --> 3* ==> 4 --> 5
> //       |                   ^
> //       \-------------------/
> //
> // and the partitioning will be {{0, 1}, {2, 3}, {4, 5}}.)
>   •	没有控制依赖关系时：假设 nodes_to_partition 是 {2, 3}，贪心划分时，图会被划分为 {{0, 1, 4}, {2, 3}, {5}}。由于数据依赖关系（-->），节点 4 可以在节点 1 之后立即执行。
> 	•	有控制依赖关系时：假设 control_edges == {{3, 4}}，节点 4 必须在节点 3 之后执行，图会被划分为 {{0, 1}, {2, 3}, {4, 5}}。
> 
>  nodes_to_partition 是一个 TfLiteIntArray 类型的对象，它包含了一组节点索引。这些节点是我们希望进行划分的重点对象。
> ```

#### 2. `simple_memory_arena.cc` 和 `simple_memory_arena.h`

这些文件实现了内存分配和管理。

```c++
/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#ifndef TENSORFLOW_LITE_SIMPLE_MEMORY_ARENA_H_
#define TENSORFLOW_LITE_SIMPLE_MEMORY_ARENA_H_

#include <cstddef>
#include <cstdint>
#include <string>
#include <vector>

#include "tensorflow/lite/core/c/common.h"

namespace tflite {

// 这个小结构保存了内存区域中动态内存分配的偏移量和大小，以及使用相应张量的 first_node 和 last_node。这意味着，在第一个节点执行操作前，需要分配具有此大小的连续内存部分，而在最后一个节点执行操作后，可以取消分配。当arena提交并设置了底层缓冲区后，分配就可以解析为实际的内存指针。
struct ArenaAllocWithUsageInterval {
  ArenaAllocWithUsageInterval() { reset(); } // 构造函数：初始化时调用 reset 方法。

  size_t offset;        // 内存块的偏移量。
  size_t size;          // 内存块的大小。
  int32_t tensor;       // 张量的索引。
  int32_t first_node;   // 使用该张量的第一个节点。
  int32_t last_node;    // 使用该张量的最后一个节点。

  inline void reset() {   // reset 方法：重置所有成员变量，清除分配信息。
    offset = 0;
    size = 0;
    tensor = -1;
    first_node = -1;
    last_node = -1;
  }

  inline bool operator<(const ArenaAllocWithUsageInterval& other) const {
    return offset < other.offset;    // operator< 方法：定义小于运算符，用于按偏移量比较两个 ArenaAllocWithUsageInterval 对象。
  }
};

struct PointerAlignedPointerPair {
  char* pointer;             //  保存未对齐和对齐的指针，便于处理对齐要求。
  char* aligned_pointer;
};

class ResizableAlignedBuffer {
 public:
  ResizableAlignedBuffer(size_t alignment, int subgraph_index)
      : buffer_{nullptr, nullptr},
        data_size_(0),
        alignment_(alignment),
        subgraph_index_(subgraph_index) {
    // To silence unused private member warning, only used with
    // TF_LITE_TENSORFLOW_PROFILER
    (void)subgraph_index_;
  }

  ~ResizableAlignedBuffer() { Release(); }

  // Resizes the buffer to make sure new_size bytes fit in the buffer. Keeps
  // alignment and any existing the data. Returns true when any external
  // pointers into the data array need to be adjusted (the buffer was moved).
  bool Resize(size_t new_size);
  // Releases any allocated memory.
  void Release();

  // Pointer to the data array.
  char* GetPtr() const { return buffer_.aligned_pointer; }
  // Size of the data array. Note: the allocated memory block might be larger
  // due to excess alignment requirements.
  size_t GetSize() const { return data_size_; }
  // Alignment of the data array.
  size_t GetAlignment() const { return alignment_; }

 private:
  ResizableAlignedBuffer(const ResizableAlignedBuffer&) = delete;
  ResizableAlignedBuffer& operator=(const ResizableAlignedBuffer&) = delete;
  ResizableAlignedBuffer(ResizableAlignedBuffer&&) = delete;
  ResizableAlignedBuffer& operator=(ResizableAlignedBuffer&&) = delete;

  PointerAlignedPointerPair buffer_;
  size_t data_size_;
  size_t alignment_;

  int subgraph_index_;
};

// This small class is responsible for allocating, deallocating and reusing
// dynamic memory from a common underlying buffer. The arena can be used in
// scenarios when the pattern of memory allocations and deallocations is
// repetitive, e.g. running NN inference in multiple iterations. Note that
// zero-sized allocations are explicitly allowed, and will resolve to null.
class SimpleMemoryArena {
 public:
  explicit SimpleMemoryArena(size_t arena_alignment, int subgraph_index = 0)
      : committed_(false),
        high_water_mark_(0),
        underlying_buffer_(arena_alignment, subgraph_index),
        active_allocs_() {}

  // Delete all allocs. This should be called when allocating the first node of
  // a subgraph.
  void ResetAllocs();

  // Delete all allocs which are deallocated before `node`. This should be
  // called before allocating tensors associated with a series of nodes. It
  // deletes allocs which are no longer required for allocating the next batch
  // of tensors. Not calling it will have no impact on the result but it may be
  // much slower.
  void PurgeActiveAllocs(int32_t node);

  // Delete all allocs which are allocated after `node`. This should be
  // called when resetting allocs after `node`. It  deletes allocs which are no
  // longer required for allocating the next batch of tensors. Not calling it
  // will have no impact on the result but it may be much slower.
  void PurgeAfter(int32_t node);

  // Calculate the active allocs at `node`. Call this if the active allocs at
  // `node` are unknown.
  void CalculateActiveAllocs(
      const std::vector<ArenaAllocWithUsageInterval>& allocs, int32_t node);

  // Schedule memory allocation for a tensor with a given size, assuming that it
  // needs to be allocated before the execution of first_node, and deallocated
  // after the execution of last_node.
  TfLiteStatus Allocate(TfLiteContext* context, size_t alignment, size_t size,
                        int32_t tensor, int32_t first_node, int32_t last_node,
                        ArenaAllocWithUsageInterval* new_alloc);

  TfLiteStatus Commit(bool* arena_reallocated);

  TfLiteStatus ResolveAlloc(TfLiteContext* context,
                            const ArenaAllocWithUsageInterval& alloc,
                            char** output_ptr);

  // This clears allocation details but does not release the underlying buffer.
  // New allocations should be committed & resolved before using this arena
  // again.
  TfLiteStatus ClearPlan();

  // This releases the underlying buffer but does not clear the allocation plan.
  // Since all associated pointers are invalidated, the arena cannot be used
  // again until Commit() is called & tensor allocations are resolved.
  TfLiteStatus ReleaseBuffer();

  size_t GetBufferSize() const { return underlying_buffer_.GetSize(); }

  std::intptr_t BasePointer() const {
    return reinterpret_cast<std::intptr_t>(underlying_buffer_.GetPtr());
  }

  // Dumps the memory allocation information of this memory arena (which could
  // be differentiated from others by the `name`) against the specified op node
  // execution plan (i.e. `execution_plan`) for the purpose of debugging.
  // Note: in order to have minimal binary increase caused by this debug info
  // dump implementation for the TfLite library, and allow users to plug-in
  // their own memory planner debugger, we have utilized weak symbols to meet
  // these two requirementsements. By default, there is no debugging info
  // dumped. To override this, provide a strong defintion of
  // tflite::DumpArenaInfo(...) whose weak defintion is in
  // simple_memory_arena.cc. TfLite provides a sample one as
  // "lite:simple_memory_arena_debug_dump". When this dep is added to the
  // program, calling this function will output information of this memory arena
  // about tenosrs and ops, such as memory arena utilization rate, live tensors
  // at each op etc.
  void DumpDebugInfo(const std::string& name,
                     const std::vector<int>& execution_plan) const;

 private:
  bool committed_;
  size_t high_water_mark_;
  ResizableAlignedBuffer underlying_buffer_;
  std::vector<ArenaAllocWithUsageInterval> active_allocs_;
};

}  // namespace tflite

#endif  // TENSORFLOW_LITE_SIMPLE_MEMORY_ARENA_H_

```



#### 3. `graph_info.cc` 和 `graph_info.h`
这些文件可能包含与计算图相关的信息和分析。

#### 4. `core/subgraph.cc` 和 `core/subgraph.h`

这些文件实现了子图的逻辑，可能与内存需求分析有关。





